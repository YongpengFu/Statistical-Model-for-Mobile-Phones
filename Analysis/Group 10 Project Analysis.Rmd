---
title: "Mobile Phones Selling Price Report"
author:
- Yongpeng Fu (10182778), Rudy Brown (10057171), Jose Palacios (30190988),
- Stuart Finley(30191070), Andrii Voitkiv(30199373)
date: "`r format(Sys.time(), '%Y-%m-%d')`"
output:

  html_document:
    toc: yes
    df_print: paged
geometry: margin=3cm
subtitle: Proposal for final project (MDSA Winter 2023)
urlcolor: blue
header-includes: \usepackage{fvextra} \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
  \usepackage[nottoc]{tocbibind}
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r echo = FALSE, message = FALSE, warning = FALSE}
library(survey) #Used for statistical analysis
library(sampling) #Used for sample data
library(ggplot2) #Used for plot
library(tibble) #used for nice display dataframe
library(magrittr) #Used for %>%
library(tidyverse) #Used to manipualte dataframe
library(dplyr) # Used for easy manipualte rows and columns
library(plyr) #Used for mapping vector values
library(stringr) #Used to manipulate string
library(gridExtra) #Used to arrange the ggplot chart
```

```{r Load Data, echo = FALSE, message = FALSE, warning = FALSE}
mobile_dataset <- as_tibble(read.csv("/Users/berg/DataspellProjects/Statistical-Model-for-Mobile-Phones/Analysis/Cleaned_Mobile_Dataset.csv"))
#Specify the level for Price_Level column
mobile_dataset$Price_Level <- factor(mobile_dataset$Price_Level, levels = c("Low", "Medium", "High", "Very High"))
```


# Chapter 5: Predicting the Selling Price of Mobile Phones
Now, after we have a better understanding of the data, we can start building a model to predict the price of a mobile phone. First, we will build a Multiple Linear Regression model to predict the price of a mobile phone to see if there are linear relationships between `Price` and dependent variables. Before we start, here are some steps we will take:
1. Split the data into training and test sets.
2. Explore collinearity between the variables.
3. Build a base valid model.
4. If there is noticable pattern in the residual plot, introduce interaction terms and higher order terms.
5. Check the assumptions of the model.
6. If any of the assumptions are violated, try to fix them with transformations and outliers removal.

At each stage of the model building process, we will check the performance of the model using the test set.

In case the multiple linear regression model does not perform well, or **the relationship is non-linear we will build a Regression Tree** that can model more complex relationships between the variables.

## Multiple Linear Regression
### Building a base model
```{r echo = FALSE, message = FALSE, warning = FALSE}
# Create a new data frame with the variables of interest
column_names_mlr <- c("Price", "Rating", "No_of_ratings", "TotalReviwes", "RamSize_inGB", "RomSize_inGB", "Is_5G" ) # "Company"
mobile_dataset_mlr <- mobile_dataset[column_names_mlr]
```
As our initial step, we will use the following variables as predictors: `Rating`, `No_of_ratings`, `TotalReviwes`, `RamSize_inGB`, `RomSize_inGB`, `Is_5G`. We will not use `Company` as a predictor because it is a categorical variable with many levels and it will make the model more complex. We will use `Price` as the response variable. Again, we will use _train_ data set to build the model and _test_ data set to evaluate the model.

```{r}
# Split the data into training and test sets
set.seed(2023)
train_index <- sample(1:nrow(mobile_dataset_mlr), 0.8 * nrow(mobile_dataset_mlr))
train_data <- mobile_dataset_mlr[train_index, ]
test_data <- mobile_dataset_mlr[-train_index, ]
```

Let's explore the relationship between the variables using the correlation matrix.
```{r message = FALSE, warning = FALSE}
# Explore collinearity between the variables
# Plot the correlation matrix with the help of the ggally package
library('GGally')
library('ggplot2')
ggpairs(train_data, lower = list(continuous = "smooth_loess", combo = "facethist", discrete = "facetbar", na = "na")) +
        scale_color_gradient(high = "red", low = "blue")
```

```{r echo = FALSE, message = FALSE, warning = FALSE}
# Build a multiple linear regression model
mlr_full_model <- lm(Price ~ Rating + No_of_ratings + TotalReviwes + RamSize_inGB + RomSize_inGB + factor(Is_5G), data = train_data)
```

Let's apply VIF test to confirm if there is any collinearity between the variables.
```{r message = FALSE, warning = FALSE}
# Check the “variance inflation factor” for each coefficient using imcdiag function from the package "regclass"
library('regclass')
VIF(mlr_full_model)
```

From the VIF values, we can see that two variables are collinear. And form the correlation matrix the relationship between the variables is also high (96%). So, we will remove the variable with the highest VIF value, that is `No_of_ratings`.

Then we update the model by removing `No_of_ratings` and check significance of the variables.

```{r}
# Update the model by removing the variable with the highest VIF value
mlr_updated_model <- lm(Price ~ Rating + TotalReviwes + RamSize_inGB + RomSize_inGB + factor(Is_5G), data = train_data)
summary(mlr_updated_model)
```
From the individual t-test, we can see that the variable `Is_5G` is not significant with p-value 82%. Therefore, we will remove it from the model.

```{r echo = FALSE, message = FALSE, warning = FALSE}
# Update the model by removing the variable with the insignificant p-value
mlr_updated_model <- lm(Price ~ Rating + TotalReviwes + RamSize_inGB + RomSize_inGB, data = train_data)
```

The adjusted R-squared value is `r summary(mlr_updated_model)$adj.r.squared`, which means that the model explains `r summary(mlr_updated_model)$adj.r.squared * 100`% of the variance in the data.

Now, all the variables are significant. Therefore, the model is valid and can be used for prediction.

```{r}
# Predict the price of the test data
test_data$predicted_price_mlr_base <- predict(mlr_updated_model, test_data)
# Negative values are not possible for the price of a mobile phone. Therefore, we will replace the negative values with 0.
test_data$predicted_price_mlr_base[test_data$predicted_price_mlr_base < 0] <- 0
# Calculate the mean square error
rmse1 <- sqrt(mean((test_data$Price - test_data$predicted_price_mlr_base)^2))
```

The RMSE value is `r rmse1`. The RMSE value is high. Therefore, we will try to improve the model by introducing interaction terms and higher order terms.

Let's plot the predicted values vs actual values to see how well the model fits the data.
```{r}
# Check assumptions of the model
# Plot the residuals vs fitted values
plot(mlr_updated_model, which = 3)
```

The residual plot shows that the model is not a good fit for the data. The residuals are not randomly distributed around the horizontal axis. There is a pattern in the residual plot. Therefore, we will try to improve the model by introducing interaction terms and higher order terms.

### Improving the model by introducing interaction terms and higher order terms
Let's improve the model by trying interaction terms.
```{r}
# Build a multiple linear regression model with interaction terms
mlr_interaction_model <- lm(Price ~ Rating + TotalReviwes + RamSize_inGB + RomSize_inGB + Rating:TotalReviwes + Rating:RamSize_inGB + Rating:RomSize_inGB + TotalReviwes:RamSize_inGB + TotalReviwes:RomSize_inGB + RamSize_inGB:RomSize_inGB, data = train_data)
summary(mlr_interaction_model)
```

Let's remove the interaction terms with insignificant p-values and build the model again. As `TotalReviwes` has the p-value higher than 0.05, we still want to keep in the model, as it is a part of the significant interaction term `TotalReviwes:RomSize_inGB`.
```{r}
# Update the model by removing the variable with the insignificant p-value
mlr_interaction_updated_model <- lm(Price ~ Rating + TotalReviwes + RamSize_inGB + RomSize_inGB + Rating:RamSize_inGB + Rating:RomSize_inGB + TotalReviwes:RomSize_inGB + RamSize_inGB:RomSize_inGB, data = train_data)
```

The adjusted R-squared value is `r summary(mlr_interaction_updated_model)$adj.r.squared`, which means that the model explains `r summary(mlr_interaction_updated_model)$adj.r.squared * 100`% of the variance in the data.

Now, all the variables are significant. Therefore, the model is valid and can be used for prediction.

```{r}
# Predict the price of the test data
test_data$predicted_price_mlr_interaction <- predict(mlr_interaction_updated_model, test_data)
# Negative values are not possible for the price of a mobile phone. Therefore, we will replace the negative values with 0.
test_data$predicted_price_mlr_interaction[test_data$predicted_price_mlr_interaction < 0] <- 0
# Calculate the mean square error
rmse2 <- sqrt(mean((test_data$Price - test_data$predicted_price_mlr_interaction)^2))
# Print the mean square error
print(paste("RMSE:", rmse2))
```

We can see that the RMSE value is higher than the base model. This is because the model is overfitting the data. Let's try to improve the model by adding polynomial terms.

We start by adding a polynomial term to the most highly correlated variable with the target variable, that is `Rating`. We start with a quadratic term and then try higher orders. We stop when some of the terms become insignificant. It turned out that the cubic term is the best fit for the model. Here is the summary of the model.

```{r}
# Build polynomial regression model
mlr_poly_model <- lm(Price ~ poly(Rating, 3) + TotalReviwes + RamSize_inGB + RomSize_inGB + Rating:RamSize_inGB + Rating:RomSize_inGB + TotalReviwes:RomSize_inGB + RamSize_inGB:RomSize_inGB, data = train_data)
```

The adjusted R-squared value is `r summary(mlr_poly_model)$adj.r.squared`, which means that the model explains `r summary(mlr_poly_model)$adj.r.squared * 100`% of the variance in the data. It is a very good fit for the data. But let's apply the model to the test data and see how well it performs.
```{r}
# Predict the price of the test data
test_data$predicted_price_mlr_poly <- predict(mlr_poly_model, test_data)
# Negative values are not possible for the price of a mobile phone. Therefore, we will replace the negative values with 0.
test_data$predicted_price_mlr_poly[test_data$predicted_price_mlr_poly < 0] <- 0
# Calculate the mean square error
rmse3 <- sqrt(mean((test_data$Price - test_data$predicted_price_mlr_poly)^2))
```

The RMSE values (`r rmse3`) slightly improved compared to the base model. Let's check the assumptions of the model.

### Checking the assumptions of the model
**Homoscedasticity** - constant variance of the residuals. The residuals should be randomly distributed around the horizontal axis.

```{r}
# Check assumptions of the model
# Plot the residuals vs fitted values
plot(mlr_poly_model, which = 3)
```

The residuals are not randomly distributed around the horizontal axis. There is a pattern in the residual plot. Therefore, we will try to improve the model by transforming the target variable. But first, let's cross-check homoskedasticity with the Breusch-Pagan test. And later check the other assumptions of the model as well.

```{r warning = FALSE, message = FALSE}
library(lmtest)
bptest(mlr_poly_model)
```

The output displays the Breusch-Pagan test that results from the MLR polynomial model. The p-value = 0 < 0.05, indicating that we reject the null hypothesis. Therefore, the test provides evidence to suggest that heteroscedasticity is present - non-constant variance.

**Normality** - the residuals should be normally distributed.
```{r}
# Normality test
par(mfrow=c(1,2))
hist(residuals(mlr_poly_model), breaks = 24)
plot(mlr_poly_model, which=2) #a Normal plot
```

The residuals are not normally distributed. QQ-plot demonstrates significant deviations from the "normal" line. Let's cross-check the normality of the residuals with the Shapiro-Wilk test.

H0: the sample data are significantly normally distributed Ha: the sample data are not significantly normally distributed
```{r}
shapiro.test(residuals(mlr_poly_model))
```

The p-value(2.2e-16) is much lower than 0.05, which confirms that the residuals are not normally distributed (reject null hypothesis).

**Linearity** - the relationship between the independent variables and the dependent variable should be linear.
```{r}
ggplot(mlr_poly_model, aes(x=.fitted, y=.resid)) +
        geom_point() + geom_smooth()+
        geom_hline(yintercept = 0)
```

The residuals deviate more and more from the horizontal axis as the fitted values increase. Therefore, the relationship between the independent variables and the dependent variable is not linear.

Therefore, we will try to fix the assumptions by transforming the target variable. We will use the Box-Cox transformation.
### Box-Cox transformation
```{r message = FALSE, warning = FALSE}
library("MASS")
bc = boxcox(mlr_poly_model,lambda=seq(-1,1))
```

```{r}
bestlambda = bc$x[which(bc$y==max(bc$y))]
mlr_bcmodel = lm((((Price^bestlambda)-1)/bestlambda) ~ poly(Rating, 3) + TotalReviwes + RamSize_inGB + RomSize_inGB + Rating:RamSize_inGB + Rating:RomSize_inGB + TotalReviwes:RomSize_inGB + RamSize_inGB:RomSize_inGB, data=train_data)
```

After the transformation and removing the outliers, we still weren't able to fix the assumptions of the model (code is not provided in the sake of brevity). Even though the adjusted R-squared value is `r summary(mlr_poly_model)$adj.r.squared` we can't use the model for prediction.

All steps of the model building process are summarized in the following diagram.

<img src="/Users/berg/DataspellProjects/Statistical-Model-for-Mobile-Phones/Analysis/Img/MLR workflow.png">

Therefore, we will try Regression Tree to model more complex relationships between the variables. And  as our final step in this chapter, we will compare the RMSE values of the models using cross-validation.


## Regression tree
```{r}
# Regression tree
# Create a new data frame with the variables of interest
column_names_tree <- c("Price", "Rating", "No_of_ratings", "TotalReviwes", "RamSize_inGB", "RomSize_inGB", "Is_5G", "Company")
mobile_dataset_tree <- mobile_dataset[column_names_tree]
# Remove missing values from the dataset
mobile_dataset_tree <- mobile_dataset_tree %>%
        filter(!is.na(Price))
# Split the data into train and test sets
set.seed(2023)
train_data <- mobile_dataset_tree[sample(1:nrow(mobile_dataset_tree), 0.7*nrow(mobile_dataset_tree)), ]
test_data <- mobile_dataset_tree[-sample(1:nrow(mobile_dataset_tree), 0.7*nrow(mobile_dataset_tree)), ]
# Fit a regression tree to the data
library(tree)
tree_model <- tree(Price ~ ., data = train_data)
summary(tree_model)
```

Our initial tree has 8 terminal nodes.
```{r}
# Plot the tree
plot(tree_model)
text(tree_model, pretty = 0)
```


```{r warning = FALSE, message = FALSE}
# Apply the tree to the test set
tree_pred <- predict(tree_model, test_data)
# Calculate the RMSE
rmse5 <- sqrt(mean((tree_pred - test_data$Price)^2))
```
The RMSE value is `r rmse5`. Let's prune the tree. To do that, we will first check the plot between the cross-valiation error and the size of the tree.

### Pruning the tree
```{r echo = FALSE, message = FALSE, warning = FALSE}
# Check the plot between the cross-valiation error and the size of the tree
cv.tree_model <- cv.tree(tree_model)
plot(cv.tree_model$size, cv.tree_model$dev, type = "b")
```

We will use 4 as the best number of terminal nodes. Let's prune the tree and calculate the RMSE.

```{r}
# Prune the tree
pruned_tree_model <- prune.tree(tree_model, best = 6)
# Plot the pruned tree
plot(pruned_tree_model)
text(pruned_tree_model, pretty = 0)
```

```{r echo = FALSE, message = FALSE, warning = FALSE}
# Apply the tree to the test set
pruned_tree_pred <- predict(pruned_tree_model, test_data)
# Calculate the RMSE
rmse6 <- sqrt(mean((pruned_tree_pred - test_data$Price)^2))
```

The RMSE after pruning is `r rmse6`.

## Cross-validation
Let's create 10 folds and calculate the cross-validation error for both models.
```{r echo = FALSE, message = FALSE, warning = FALSE}
# Create 10 folds
library(caret)
folds<-createFolds(mobile_dataset_tree$Price, k=10)
```

```{r echo = FALSE, message = FALSE, warning = FALSE}
# Calculate the cross-validation error for linear regression
library(MASS)
cv_error_lr <- c()
for (i in 1:10){
  train_lr <- mobile_dataset_mlr[-folds[[i]], ]
  test_lr <- mobile_dataset_mlr[folds[[i]], ]
  lr_model <- mlr_poly_model
  lr_pred <- predict(lr_model, test_lr)
  RMSE <- sqrt(mean((lr_pred - test_lr$Price)^2))
  cv_error_lr <- c(cv_error_lr, RMSE)
}
cv_error_lr <- mean(cv_error_lr)
# print(paste("Cross-validation error for linear regression:", cv_error_lr))
```

Cross-validation error for our best linear regression model is `r cv_error_lr`.

```{r echo = FALSE, warning = FALSE, message = FALSE}
# Calculate the cross-validation error for regression tree
cv_error_tree <- c()
for (i in 1:10){
  train_cv <- mobile_dataset_tree[-folds[[i]], ]
  test_cv <- mobile_dataset_tree[folds[[i]], ]
  tree_model <- pruned_tree_model # tree(Price ~ ., data = train_cv)
  tree_pred <- predict(tree_model, test_cv)
  RMSE <- sqrt(mean((tree_pred - test_cv$Price)^2))
  cv_error_tree <- c(cv_error_tree, RMSE)
}
cv_error_tree <- mean(cv_error_tree)
# print(paste("Cross-validation error for regression tree:", cv_error_tree))
```

Cross-validation error for our best linear regression model is `r cv_error_tree`.

## Conclusion for chapter 5
In this chapter, we have built a linear regression model and a regression tree model to predict the price of a mobile phone. We have used the RMSE value and cross-validation method to compare the models. The RMSE value for the linear regression model is `r rmse3` and the RMSE value for the regression tree model is `r rmse6`. Therefore, the regression tree model is better than the linear regression model. Besides, the Multiple Linear Regression model doesn't meet the assumptions. Comparing to the Regression Tree model, it doesn't have such assumptions. Therefore, we recommend using the Regression Tree model for prediction.
